{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Similarity task\n",
    "\n",
    "In this notebook, we demonstrate how to use a trained syntactic rand-walk model for the phrase similarity task introduced by Mitchell and Lapata in [this paper](https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6709.2010.01106.x). The dataset is available [here](http://homepages.inf.ed.ac.uk/s0453356/share). We compare various composition techniques based on the syntactic rand-walk model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensor_operations as to\n",
    "from collections import defaultdict\n",
    "from scipy import linalg as la\n",
    "from scipy.stats import spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set paths to important files\n",
    "vocab_file = \"../../datasets/rw_vocab_no_stopwords.txt\"\n",
    "embedding_file = \"../../datasets/rw_vectors.txt\"\n",
    "param_file = \"/usr/xtmp/abef/learned_params_dep_an_rw.npz\"\n",
    "phrase_file = \"../../datasets/phrase_sim.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in the vocab, create mapping from word to index\n",
    "vocab = []\n",
    "with open(vocab_file,\"r\") as f:\n",
    "    for line in f:\n",
    "        vocab.append(line.strip(\"\\n\"))\n",
    "vocab_dict = defaultdict(lambda : -1) # this will return index -1 if key not found\n",
    "for i, w in enumerate(vocab):\n",
    "    vocab_dict[w] = i\n",
    "    \n",
    "# load in the word embeddings, compute norm of each embedding\n",
    "vectors = np.loadtxt(embedding_file)\n",
    "norms = la.norm(vectors,axis=1)\n",
    "\n",
    "# load in the learned composition tensor\n",
    "params = np.load(param_file)\n",
    "T = params[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in phrase similarity dataset\n",
    "phrases_all = pd.read_csv(phrase_file, sep=\" \")\n",
    "phrases = phrases_all.loc[phrases_all[\"type\"]==\"adjectivenouns\"] # only use adjective-noun phrases\n",
    "\n",
    "# get word frequencies -- used in sif composition\n",
    "freqs = {}\n",
    "tot = 0\n",
    "with open(\"../../datasets/semvec_2015/enwiki_vocab.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        toks = line.strip().split(\" \")\n",
    "        freqs[toks[0]] = int(toks[1])\n",
    "        tot += int(toks[1])\n",
    "for item in freqs.items():\n",
    "    freqs[item[0]] /= tot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(v1, v2):\n",
    "    \"\"\"\n",
    "    Return the cosine similarity of two vectors\n",
    "    \"\"\"\n",
    "    return np.dot(v1,v2)/np.sqrt(np.dot(v1,v1)*np.dot(v2,v2))\n",
    "\n",
    "def get_additive_score(vectors, phrase_df, vocab_dict, b):\n",
    "    \"\"\"\n",
    "    Compute the spearman and pearson correlations between phrases\n",
    "    according to the additive composition with weight parameter b,\n",
    "    i.e. if v_a and v_n are the embeddings for the adjective and \n",
    "    noun, respectively, then use the composition b*v_a + v_n.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vectors : ndarray of shape (N,d)\n",
    "       The word embeddings (one embedding per row)\n",
    "    phrase_df : pandas DataFrame containing the phrase pairs and human similarities\n",
    "    vocab_dict : dictionary that maps word to index\n",
    "    b : nonnegative float\n",
    "       The weighting parameter for the adjective embedding in the composition\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (spear, pears) : the two similarity scores computed by the embeddings\n",
    "    \"\"\"\n",
    "    score = []\n",
    "    sim = []\n",
    "    for i in range(phrase_df.shape[0]):\n",
    "        a1 = phrase_df.iloc[i,3]\n",
    "        n1 = phrase_df.iloc[i,4]\n",
    "        a2 = phrase_df.iloc[i,5]\n",
    "        n2 = phrase_df.iloc[i,6]\n",
    "\n",
    "        a1_ind = vocab_dict[a1]\n",
    "        n1_ind = vocab_dict[n1]\n",
    "        a2_ind = vocab_dict[a2]\n",
    "        n2_ind = vocab_dict[n2]\n",
    "\n",
    "        p1 = b*vectors[a1_ind]+vectors[n1_ind]\n",
    "        p2 = b*vectors[a2_ind]+vectors[n2_ind]\n",
    "        \n",
    "        sim.append(phrase_df.iloc[i,7])\n",
    "        score.append(cosine(p1,p2))\n",
    "    return spearmanr(score,sim)[0],pearsonr(score,sim)[0]\n",
    "\n",
    "def get_tensor_score(T,vectors, phrase_df, vocab_dict, b, vectors_T = None, a=1):\n",
    "    \"\"\"\n",
    "    Compute the spearman and pearson correlations between phrases\n",
    "    according to the tensor composition with weight parameters a and b,\n",
    "    i.e. if v_a and v_n are the embeddings for the adjective and \n",
    "    noun, respectively, then use the composition a*v_a + v_n + b*T(v_a,v_n,.).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : ndarray of shape (d,d,d)\n",
    "       The composition tensor\n",
    "    vectors : ndarray of shape (N,d)\n",
    "       The word embeddings (one embedding per row)\n",
    "    phrase_df : pandas DataFrame containing the phrase pairs and human similarities\n",
    "    vocab_dict : dictionary that maps word to index\n",
    "    b : nonnegative float\n",
    "       The weighting parameter for the tensor component\n",
    "    a : nonnegative float (optional, defaults to 1)\n",
    "       The weighting parameter for the adjective embedding\n",
    "    vectors_T : ndarray of shape (N,d) (optional, defaults to None)\n",
    "       Gives an optional set of embeddings used specifically in the tensor component,\n",
    "       If None, the input vectors is used for tensor component as well. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (spear, pears) : the two similarity scores computed by the embeddings\n",
    "    \"\"\"\n",
    "    score = []\n",
    "    sim = []\n",
    "    if not vectors_T:\n",
    "        vectors_T = vectors\n",
    "        \n",
    "    for i in range(phrase_df.shape[0]):\n",
    "        a1 = phrase_df.iloc[i,3]\n",
    "        n1 = phrase_df.iloc[i,4]\n",
    "        a2 = phrase_df.iloc[i,5]\n",
    "        n2 = phrase_df.iloc[i,6]\n",
    "\n",
    "        a1_ind = vocab_dict[a1]\n",
    "        n1_ind = vocab_dict[n1]\n",
    "        a2_ind = vocab_dict[a2]\n",
    "        n2_ind = vocab_dict[n2]\n",
    "\n",
    "        p1 = a*vectors[a1_ind]+vectors[n1_ind] \n",
    "        t1 = b*to.bilinear_lowrank_batch_np(T,vectors_T[a1_ind],vectors_T[n1_ind])\n",
    "        p2 = a*vectors[a2_ind]+vectors[n2_ind] \n",
    "        t2 = b*to.bilinear_lowrank_batch_np(T,vectors_T[a2_ind],vectors_T[n2_ind])\n",
    "\n",
    "        sim.append(phrase_df.iloc[i,7])\n",
    "        score.append(cosine(p1+t1,p2+t2))\n",
    "    return spearmanr(score,sim)[0],pearsonr(score,sim)[0]\n",
    "\n",
    "def get_sif_score(vectors, phrase_df, vocab_dict, freqs, A=1e-3): \n",
    "    \"\"\"\n",
    "    Compute the spearman and pearson correlations between phrases\n",
    "    according to the sif composition with parameter A.\n",
    "    See the paper \"A simple but tough-to-beat baseline for sentence embeddings\"\n",
    "    by Arora et al. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vectors : ndarray of shape (N,d)\n",
    "       The word embeddings (one embedding per row)\n",
    "    phrase_df : pandas DataFrame containing the phrase pairs and human similarities\n",
    "    vocab_dict : dictionary that maps word to index\n",
    "    freqs : a dictionary that maps each word to its relative frequency\n",
    "    A : nonnegative float (optional, defaults to 1e-3)\n",
    "       The smoothing parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (spear, pears) : the two similarity scores computed by the embeddings\n",
    "    \"\"\"\n",
    "    score = []\n",
    "    sim = []\n",
    "    sif_embeddings = []\n",
    "    X = np.zeros((300,2*phrase_df.shape[0]))\n",
    "    for i in range(phrase_df.shape[0]):\n",
    "        a1 = phrase_df.iloc[i,3]\n",
    "        n1 = phrase_df.iloc[i,4]\n",
    "        a2 = phrase_df.iloc[i,5]\n",
    "        n2 = phrase_df.iloc[i,6]\n",
    "\n",
    "        a1_ind = vocab_dict[a1]\n",
    "        n1_ind = vocab_dict[n1]\n",
    "        a2_ind = vocab_dict[a2]\n",
    "        n2_ind = vocab_dict[n2]\n",
    "\n",
    "        p1 = (A/(A+freqs[a1]))*vectors[a1_ind]+(A/(A+freqs[n1]))*vectors[n1_ind]\n",
    "        p2 = (A/(A+freqs[a2]))*vectors[a2_ind]+(A/(A+freqs[n2]))*vectors[n2_ind]\n",
    "        \n",
    "        X[:,i] = p1\n",
    "        X[:,phrase_df.shape[0]+i] = p2\n",
    "        sif_embeddings.append([p1,p2])\n",
    "\n",
    "        sim.append(phrase_df.iloc[i,7])\n",
    "    u = la.svd(X)[0][:,0] # get top left singular vector of X\n",
    "\n",
    "    for pair in sif_embeddings:\n",
    "        for i in range(2):\n",
    "            pair[i] = pair[i] - np.dot(pair[i],u)*u\n",
    "\n",
    "    score=[]\n",
    "    for pair in sif_embeddings:\n",
    "        score.append(cosine(pair[0],pair[1]))\n",
    "    return spearmanr(score,sim)[0],pearsonr(score,sim)[0]\n",
    "\n",
    "def get_sif_tensor_score(T,vectors, phrase_df, vocab_dict, b, freqs, a=1, A=1e-3, vectors_T=None):\n",
    "    \"\"\"\n",
    "    Compute the spearman and pearson correlations between phrases\n",
    "    according to the sif composition with tensor component.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : ndarray of shape (d,d,d)\n",
    "       The composition tensor\n",
    "    vectors : ndarray of shape (N,d)\n",
    "       The word embeddings (one embedding per row)\n",
    "    phrase_df : pandas DataFrame containing the phrase pairs and human similarities\n",
    "    vocab_dict : dictionary that maps word to index\n",
    "    b : nonnegative float\n",
    "       The weighting parameter for the tensor component\n",
    "    freqs : a dictionary that maps each word to its relative frequency\n",
    "    a : nonnegative float (optional, defaults to 1)\n",
    "       The weighting parameter for the sif embedding\n",
    "    A : nonnegative float (optional, defaults to 1e-3)\n",
    "       The smoothing parameter\n",
    "    vectors_T : ndarray of shape (N,d) (optional, defaults to None)\n",
    "       Gives an optional set of embeddings used specifically in the tensor component,\n",
    "       If None, the input vectors is used for tensor component as well. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (spear, pears) : the two similarity scores computed by the embeddings\n",
    "    \"\"\"\n",
    "    score = []\n",
    "    sim = []\n",
    "    sif_embeddings = []\n",
    "    tensor_comp = []\n",
    "    X = np.zeros((300,2*phrase_df.shape[0]))\n",
    "    if vectors_T is None:\n",
    "        vectors_T=vectors\n",
    "    for i in range(phrase_df.shape[0]):\n",
    "        a1 = phrase_df.iloc[i,3]\n",
    "        n1 = phrase_df.iloc[i,4]\n",
    "        a2 = phrase_df.iloc[i,5]\n",
    "        n2 = phrase_df.iloc[i,6]\n",
    "\n",
    "        a1_ind = vocab_dict[a1]\n",
    "        n1_ind = vocab_dict[n1]\n",
    "        a2_ind = vocab_dict[a2]\n",
    "        n2_ind = vocab_dict[n2]\n",
    "\n",
    "        p1 = a*(A/(A+freqs[a1]))*vectors[a1_ind]+(A/(A+freqs[n1]))*vectors[n1_ind] \n",
    "        t1 = (A/(A+freqs[n1]))*(A/(A+freqs[a1]))*b \\\n",
    "            *to.bilinear_lowrank_batch_np(T,vectors_T[a1_ind],vectors_T[n1_ind])\n",
    "        p2 = a*(A/(A+freqs[a2]))*vectors[a2_ind]+(A/(A+freqs[n2]))*vectors[n2_ind] \n",
    "        t2 = (A/(A+freqs[n2]))*(A/(A+freqs[a2]))*b \\\n",
    "            *to.bilinear_lowrank_batch_np(T,vectors_T[a2_ind],vectors_T[n2_ind])\n",
    "\n",
    "        X[:,i] = p1\n",
    "        X[:,phrase_df.shape[0]+i] = p2\n",
    "        sif_embeddings.append([p1,p2])\n",
    "        tensor_comp.append([t1,t2])\n",
    "\n",
    "        sim.append(phrase_df.iloc[i,7])\n",
    "        score.append(cosine(p1,p2))\n",
    "    u = la.svd(X)[0][:,0] # get top left singular vector of X\n",
    "\n",
    "    for pair in sif_embeddings:\n",
    "        for i in range(2):\n",
    "            pair[i] = pair[i] - np.dot(pair[i],u)*u\n",
    "\n",
    "    score=[]\n",
    "    for p,t in zip(sif_embeddings,tensor_comp):\n",
    "        score.append(cosine(p[0]+t[0],p[1]+t[1]))\n",
    "        \n",
    "    return spearmanr(score,sim)[0],pearsonr(score,sim)[0]\n",
    "\n",
    "def get_best_additive_param(vectors, phrase_df, vocab_dict):\n",
    "    \"\"\"\n",
    "    Range over several parameter values for additive composition,\n",
    "    return the results for each one.\n",
    "    \"\"\"\n",
    "    b_params = np.linspace(0,2,21)\n",
    "    b_results = np.zeros((2,len(b_params)))\n",
    "    for k,b in enumerate(b_params):\n",
    "        s,p = get_additive_score(vectors,phrase_df,vocab_dict,b)\n",
    "        b_results[0,k] = s\n",
    "        b_results[1,k] = p\n",
    "    return b_results,b_params\n",
    "\n",
    "def get_best_tensor_param(T,vectors, phrase_df, vocab_dict, a=1, vectors_T=None):\n",
    "    \"\"\"\n",
    "    Range over several parameter values for tensor composition,\n",
    "    return the results for each one.\n",
    "    \"\"\"\n",
    "    b_params = np.linspace(0,1,11)\n",
    "    b_results = np.zeros((2,len(b_params)))\n",
    "    for k,b in enumerate(b_params):\n",
    "        s,p = get_tensor_score(T,vectors,phrase_df,vocab_dict,b,a=a,vectors_T=vectors_T)\n",
    "        b_results[0,k] = s\n",
    "        b_results[1,k] = p\n",
    "    return b_results,b_params\n",
    "\n",
    "def get_best_sif_tensor_param(T,vectors, phrase_df, vocab_dict, freqs, a=1, vectors_T=None):\n",
    "    \"\"\"\n",
    "    Range over several parameter values for sif+tensor composition,\n",
    "    return the results for each one.\n",
    "    \"\"\"\n",
    "    b_params = np.linspace(0,.5,11)\n",
    "    b_results = np.zeros((2,len(b_params)))\n",
    "    for k,b in enumerate(b_params):\n",
    "        s,p = get_sif_tensor_score(T,vectors,phrase_df,vocab_dict,b,freqs,a=a,vectors_T=vectors_T)\n",
    "        b_results[0,k] = s\n",
    "        b_results[1,k] = p\n",
    "    return b_results,b_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "best weighted1 params: 0.7/0.5\n",
      "best weighted2 params: 0.7/0.6\n",
      "best tensor params: 0.3/0.3\n",
      "best sif tensor params: 0.05/0.25\n",
      "\n",
      "Fold 1\n",
      "best weighted1 params: 0.7/0.7\n",
      "best weighted2 params: 0.7/0.5\n",
      "best tensor params: 0.3/0.3\n",
      "best sif tensor params: 0.05/0.1\n",
      "\n",
      "Fold 2\n",
      "best weighted1 params: 0.7/0.5\n",
      "best weighted2 params: 0.7/0.6\n",
      "best tensor params: 0.4/0.3\n",
      "best sif tensor params: 0.05/0.2\n",
      "[[0.44687226 0.43851155]\n",
      " [0.45194553 0.45118495]\n",
      " [0.45194553 0.45367238]\n",
      " [0.46171781 0.47070609]\n",
      " [0.48296984 0.47827045]\n",
      " [0.48432109 0.47940004]]\n"
     ]
    }
   ],
   "source": [
    "# split the participants randomly into 3 disjoint groups, use two groups as development set\n",
    "# on development set, select best composition params\n",
    "# on test set, use the selected composition params to evaluate the composition techniques\n",
    "\n",
    "participants = list(set(phrases[\"participant\"]))\n",
    "perm = np.random.permutation(len(participants))\n",
    "results = np.zeros((3,6,2))\n",
    "\n",
    "for fold in range(3):\n",
    "    print(\"\\nFold {}\".format(fold))\n",
    "    # split into dev and test sets\n",
    "    dev_participants = [participants[i] for i in perm[18*fold:18*(fold+1)]]\n",
    "    phrases_dev = phrases.loc[phrases.apply(lambda x: x[0] in dev_participants, axis=1)]\n",
    "    phrases_test = phrases.loc[~phrases.apply(lambda x: x[0] in dev_participants,axis=1)]\n",
    "\n",
    "    # compute score for plain additive composition (no weighting)\n",
    "    add_scores = get_additive_score(vectors, phrases_test,vocab_dict,1)\n",
    "    results[fold,0,:] = np.array(add_scores)\n",
    "\n",
    "    # compute score for weighted additive composition, selecting parameter on dev set\n",
    "    a_results,a_params = get_best_additive_param(vectors, phrases_dev, vocab_dict)\n",
    "    a_spear,a_pears = [a_params[j] for j in a_results.argmax(axis=1)]\n",
    "    res_spear,res_pears = get_additive_score(vectors,phrases_test,vocab_dict,a_spear)\n",
    "    if a_spear != a_pears:\n",
    "        res_pears = get_additive_score(vectors,phrases_test,vocab_dict,a_pears)[1]\n",
    "    results[fold,1,:] = np.array([res_spear,res_pears])\n",
    "    print(\"best weighted1 params: {}/{}\".format(round(a_spear,2),round(a_pears,2)))\n",
    "\n",
    "    # compute score for weighted additive composition, selecting parameter on test set\n",
    "    a_results,a_params = get_best_additive_param(vectors, phrases_test, vocab_dict)\n",
    "    a_spear,a_pears = [a_params[j] for j in a_results.argmax(axis=1)]\n",
    "    res_spear,res_pears = get_additive_score(vectors,phrases_test,vocab_dict,a_spear)\n",
    "    if a_spear != a_pears:\n",
    "        res_pears = get_additive_score(vectors,phrases_test,vocab_dict,a_pears)[1]\n",
    "    results[fold,2,:] = np.array([res_spear,res_pears])\n",
    "    print(\"best weighted2 params: {}/{}\".format(round(a_spear,2),round(a_pears,2)))\n",
    "\n",
    "    # compute score for tensor composition, selecting parameter on dev set\n",
    "    # use .6 for default weighting parameter a since this is a good weighted additive param\n",
    "    b_results, b_params = get_best_tensor_param(T,vectors,phrases_dev,vocab_dict,a=.6)\n",
    "    b_spear,b_pears = [b_params[j] for j in b_results.argmax(axis=1)]\n",
    "    res_spear, res_pears = get_tensor_score(T,vectors,phrases_test,vocab_dict,b_spear,a=.6)\n",
    "    if b_pears != b_spear:\n",
    "        res_pears = get_tensor_score(T,vectors,phrases_test,vocab_dict,b_pears,a=.6)[1]\n",
    "    results[fold,3,:] = np.array([res_spear,res_pears])\n",
    "    print(\"best tensor params: {}/{}\".format(round(b_spear,2),round(b_pears,2)))\n",
    "\n",
    "    # compute score for sif embedding\n",
    "    sif_scores = get_sif_score(vectors, phrases_test,vocab_dict,freqs)\n",
    "    results[fold,4,:] = np.array(sif_scores)\n",
    "\n",
    "    # compute score for sif+tensor embeddings, selecting parameter on dev set\n",
    "    c_results, c_params = get_best_sif_tensor_param(T,vectors, phrases_dev, vocab_dict, freqs)\n",
    "    c_spear,c_pears = [c_params[j] for j in c_results.argmax(axis=1)]\n",
    "    res_spear, res_pears = get_sif_tensor_score(T,vectors,phrases_test,vocab_dict,c_spear,freqs)\n",
    "    if c_spear != c_pears:\n",
    "        res_pears = get_sif_tensor_score(T,vectors,phrases_test,vocab_dict,c_pears,freqs)[1]\n",
    "    results[fold,5,:] = np.array([res_spear,res_pears])\n",
    "    print(\"best sif tensor params: {}/{}\".format(round(c_spear,2),round(c_pears,2)))\n",
    "\n",
    "print(results.mean(axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
