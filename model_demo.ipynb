{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the trained model\n",
    "\n",
    "The main use of the syntactic rand-walk model is to create composite embeddings for syntactically-related pairs of words. Below, we demonstrate how to do this using a model trained specifically for adjective-noun pairs. \n",
    "\n",
    "After training the syntactic rand-walk model using the file train_from_triplecounts.py, you should have a numpy archive file containing the learned parameters. You should also have a text file containing the word embeddings used in the training, where line i gives the embedding of word i as a space-separated list of floating point numbers. Finally, you should have a file that gives the vocabulary for the word embeddings set, where line i contains word i. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensor_operations as to\n",
    "from collections import defaultdict\n",
    "from scipy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set paths to important files\n",
    "vocab_file = \"../../datasets/rw_vocab_no_stopwords.txt\"\n",
    "embedding_file = \"../../datasets/rw_vectors.txt\"\n",
    "param_file = \"/usr/xtmp/abef/learned_params_dep_an_rw.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the vocab, create mapping from word to index\n",
    "vocab = []\n",
    "with open(vocab_file,\"r\") as f:\n",
    "    for line in f:\n",
    "        vocab.append(line.strip(\"\\n\"))\n",
    "vocab_dict = defaultdict(lambda : -1) # this will return index -1 if key not found\n",
    "for i, w in enumerate(vocab):\n",
    "    vocab_dict[w] = i\n",
    "    \n",
    "# load in the word embeddings, compute norm of each embedding\n",
    "vectors = np.loadtxt(embedding_file)\n",
    "norms = la.norm(vectors,axis=1)\n",
    "\n",
    "# load in the learned composition tensor\n",
    "params = np.load(param_file)\n",
    "T = params[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# suppose word 0 and word 1 form an adjective-noun pair\n",
    "av = vectors[0] # adjective vector\n",
    "nv = vectors[1] # noun vector\n",
    "composite_embedding = av + nv + to.bilinear_lowrank_batch_np(T,av,nv).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">national park</th>\n",
       "      <th colspan=\"2\" halign=\"left\">artificial intelligence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">european union</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>additive</th>\n",
       "      <th>tensor</th>\n",
       "      <th>additive</th>\n",
       "      <th>tensor</th>\n",
       "      <th>additive</th>\n",
       "      <th>tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>park</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>european</td>\n",
       "      <td>annexes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>park</td>\n",
       "      <td>pisgah</td>\n",
       "      <td>artificial</td>\n",
       "      <td>artificial</td>\n",
       "      <td>union</td>\n",
       "      <td>eec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parks</td>\n",
       "      <td>adjoins</td>\n",
       "      <td>researchers</td>\n",
       "      <td>gchq</td>\n",
       "      <td>europe</td>\n",
       "      <td>stabilisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>historic</td>\n",
       "      <td>exmoor</td>\n",
       "      <td>human</td>\n",
       "      <td>computational</td>\n",
       "      <td>federation</td>\n",
       "      <td>cooperation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest</td>\n",
       "      <td>geopark</td>\n",
       "      <td>perception</td>\n",
       "      <td>researchers</td>\n",
       "      <td>nations</td>\n",
       "      <td>precondition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recreation</td>\n",
       "      <td>cays</td>\n",
       "      <td>computational</td>\n",
       "      <td>robotics</td>\n",
       "      <td>countries</td>\n",
       "      <td>iea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>monument</td>\n",
       "      <td>gunung</td>\n",
       "      <td>knowledge</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>cooperation</td>\n",
       "      <td>saarc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wildlife</td>\n",
       "      <td>wildlife</td>\n",
       "      <td>communication</td>\n",
       "      <td>cybernetics</td>\n",
       "      <td>unions</td>\n",
       "      <td>eurozone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>preserve</td>\n",
       "      <td>otway</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>cognition</td>\n",
       "      <td>soviet</td>\n",
       "      <td>cpsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reserve</td>\n",
       "      <td>badlands</td>\n",
       "      <td>cognition</td>\n",
       "      <td>minds</td>\n",
       "      <td>socialist</td>\n",
       "      <td>insofar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  national park           artificial intelligence                 \\\n",
       "       additive    tensor                additive         tensor   \n",
       "0      national      park            intelligence   intelligence   \n",
       "1          park    pisgah              artificial     artificial   \n",
       "2         parks   adjoins             researchers           gchq   \n",
       "3      historic    exmoor                   human  computational   \n",
       "4        forest   geopark              perception    researchers   \n",
       "5    recreation      cays           computational       robotics   \n",
       "6      monument    gunung               knowledge    intelligent   \n",
       "7      wildlife  wildlife           communication    cybernetics   \n",
       "8      preserve     otway             intelligent      cognition   \n",
       "9       reserve  badlands               cognition          minds   \n",
       "\n",
       "  european union                 \n",
       "        additive         tensor  \n",
       "0       european        annexes  \n",
       "1          union            eec  \n",
       "2         europe  stabilisation  \n",
       "3     federation    cooperation  \n",
       "4        nations   precondition  \n",
       "5      countries            iea  \n",
       "6    cooperation          saarc  \n",
       "7         unions       eurozone  \n",
       "8         soviet           cpsu  \n",
       "9      socialist        insofar  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the nearest words to a given set of adjective-noun pairs\n",
    "# compare the standard additive composition with our tensor composition\n",
    "# use cosine similarity\n",
    "N = 10\n",
    "phrases = [\"national park\",\"artificial intelligence\",\"european union\"]\n",
    "res_ind = pd.MultiIndex.from_product([phrases, [\"additive\",\"tensor\"]])\n",
    "res = pd.DataFrame(data = np.zeros((N,len(phrases)*2)),columns=res_ind)\n",
    "\n",
    "for p in phrases:\n",
    "    a = p.split(\" \")[0]\n",
    "    n = p.split(\" \")[1]\n",
    "    av = vectors[vocab_dict[a]]\n",
    "    nv = vectors[vocab_dict[n]]\n",
    "    \n",
    "    c1 = av + nv\n",
    "    c2 = av + nv + to.bilinear_lowrank_batch_np(T,av,nv).flatten()\n",
    "    \n",
    "    topwords = [vocab[i] for i in np.argsort(np.dot(vectors/norms[:,None],c1))[::-1][:N]]\n",
    "    res[p,\"additive\"] = topwords\n",
    "    topwords = [vocab[i] for i in np.argsort(np.dot(vectors/norms[:,None],c2))[::-1][:N]]\n",
    "    res[p,\"tensor\"]=topwords\n",
    "    \n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
